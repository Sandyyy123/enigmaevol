#!/usr/bin/env Rscript
args = commandArgs(trailingOnly=TRUE)
#
# This script will...
# 1- Merge clumped files.
# 2- Read control variants generated by SNPsnap.
# 3- Run plink on control variants to expand control sets.
# 4- Pick random variants in LD with controls.
#
# Run this script with R 4.0.3
#
# Gokberk Alagoz
# created on: 02.06.2021

#####

library(biomaRt)
library(data.table)
options(stringsAsFactors=FALSE)

#--------------------------------------------
# PATHS

CSAregions = "/data/workspaces/lag/workspaces/lg-ukbiobank/projects/enigma_evol/enigma_evo/evolution/results/selection_analysis/european_hemave/CSAregions"
outDir = "/data/workspaces/lag/workspaces/lg-ukbiobank/projects/enigma_evol/enigma_evo/evolution/results/selection_analysis/european_hemave"
resources = "/data/workspaces/lag/workspaces/lg-ukbiobank/projects/enigma_evol/enigma_evo/evolution/resources/"
annots=c("/data/workspaces/lag/workspaces/lg-ukbiobank/projects/enigma_evol/enigma_evo/evolution/resources/primate_phyloP",
         "/data/workspaces/lag/workspaces/lg-ukbiobank/projects/enigma_evol/enigma_evo/evolution/resources/primate_phastCons")

#--------------------------------------------
# FUNCTIONS

df2bed = function(CSAregions, outDir) {
  # extracts phyloP and phastCons scores from .bed files
  # for each SNP
  
  for (i in dir(CSAregions, full.names = T)) {
    fileName = sapply(strsplit(i, split = "/"), tail, 1)
    tmp_df = read.table(i)
    tmp_bed = data.frame(chr = character(), 
                         str = character(),
                         end = character(),
                         id = character(),
                         set = character(),
                         stringsAsFactors = F)
    m = 1
        for (j in 1:nrow(tmp_df)){
            for (k in 1:ncol(tmp_df)){
              tmp_bed[m,]$chr = paste0("chr", strsplit(tmp_df[j,k], split = ":")[[1]][1])
              tmp_bed[m,]$str = as.numeric(strsplit(tmp_df[j,k], split = ":")[[1]][2])-1
              tmp_bed[m,]$end = as.numeric(strsplit(tmp_df[j,k], split = ":")[[1]][2])
              tmp_bed[m,]$id = m
              tmp_bed[m,]$set = j
              m = m + 1
              }
        }
    
    # Clear NAs
    tmp_bed = na.omit(tmp_bed)
    
    write.table(tmp_bed, file = paste0(outDir, "/variant_beds/", fileName), 
                quote = F, row.names = F, col.names = F, sep = "\t")
  }
  
}

sort_bed = function(outDir) {
  # This function intersects annotation and variant
  # bed files using bedtools.
  
  for (i in dir(paste0(outDir,"/variant_beds"), full.names = T)) {
    
    fileName = sapply(strsplit(i, split = "/"), tail, 1)
    rsID = strsplit(fileName, split = "\\.")[[1]][1]
    system(paste0("module load bedtools/2.29.2 \
                  sort-bed ", i, " > ", outDir, "/variant_beds/", rsID,
                  ".sorted.bed"))
    tmp = read.table(paste0(outDir, "/variant_beds/", rsID, ".sorted.bed"))
    tmp$V4 = rownames(tmp)
    write.table(tmp, file = paste0(outDir, "/variant_beds/", rsID, ".sorted.bed"), 
                quote = F, row.names  = F, col.names = F, sep = "\t")
  }
}

intersect_varBed_vs_annotBed = function(annots, outDir) { #TODO this takes ages. parallelize over chr and lead SNPs.
  # This function intersects annotation and variant
  # bed files using bedtools.
  for (annot_folder in annots) {
    
    annot_name = strsplit(annot_folder, split = "_")[[1]][4]
    for (i in dir(paste0(outDir,"/variant_beds"), full.names = T, pattern = "sorted")) {
      
      fileName = strsplit(i, split = "/")[[1]][15]
      rsID = strsplit(fileName, split = "\\.")[[1]][1]
      system(paste0("module load bedtools/2.29.2 \
                  cd ", annot_folder," \
                  for i in `seq 1 22` X Y; \
                    do \
                      annot=\"chr${i}.",annot_name,"46way.primate.starch\"; \
                      bedmap --echo --mean --chrom chr${i} ", i, " ", annot_folder, "/${annot}",
                    " > ", outDir, "/intersects/", rsID, "_chr${i}", annot_name,
                    ".intersect.bed; \
                    done"))
    }
  }
}


merge_sort_filter = function(CSAregions, outDir) {
  
  for (i in dir(CSAregions)) {
    tmp_lead_snp = strsplit(i, split = "\\.")[[1]][1]
    
    #for (annot_folder in annots) {
      annot_name = strsplit(annot_folder, split = "_")[[1]][4]
      
      # merge all chromosomes for each lead snp (ignore X and Y chrs)
      for (j in 1:22) {
        system(paste0("cat ", outDir, "/intersects/", tmp_lead_snp, "_chr", j, 
                      annot_name, ".intersect.bed >> ", outDir, "/intersects/", 
                      tmp_lead_snp, ".allChr.", 
                      annot_name, ".intersect.bed"))
      }

      # read the merged bed file
      tmp_bed = read.table(paste0(outDir, "/intersects/", tmp_lead_snp, 
                                  ".allChr.", annot_name, ".intersect.bed"))
      
      # split the locus index and evol. measure column (index|measure) into 2 columns
      for (k in 1:nrow(tmp_bed)) {
        tmp_bed$V6[k] = as.numeric(strsplit(tmp_bed$V5[k], split = "\\|")[[1]][2])
        tmp_bed$V5[k] = as.numeric(strsplit(tmp_bed$V5[k], split = "\\|")[[1]][1])
      }

      # sort snps based on which loci they come from
      tmp_bed = tmp_bed[order(tmp_bed$V5),]
      rownames(tmp_bed) = 1:nrow(tmp_bed)
      
      # count LD-buddies of each lead snp which are not NaN
      lead_snp_nonNaN_ldbuddy_count = nrow(tmp_bed[tmp_bed$V5==1,]) - sum(is.na(tmp_bed[tmp_bed$V5==1,]$V6))

      drop_count = 0
      # iterate over control sets. if a locus' coverage is less than 90%, drop that locus.
      for (l in levels(as.factor(tmp_bed$V5))) {
        tmp_snp_coverage =  (nrow(tmp_bed[tmp_bed$V5==l,]) - sum(is.na(tmp_bed[tmp_bed$V5==l,]$V6))) / lead_snp_nonNaN_ldbuddy_count
        
        if (tmp_snp_coverage <= 0.9 && nrow(tmp_bed[tmp_bed$V5==l,]) != 0) {
          # no need to drop these control regions! only count them.
          #tmp_bed = tmp_bed[-c(as.integer(rownames(tmp_bed[tmp_bed$V5==l,]))),]
          drop_count = drop_count + 1
        }
      }
      
      # if more than 60% of control loci are dropped, don't use that lead snp
      if ((drop_count / length(levels(as.factor(tmp_bed$V5)))) >= 0.6) {
        tmp_bed = print(paste0("Too many control regions have missing evol. measures,
                        Thus the lead SNP itself was dropped."))
      }
      
      write.table(tmp_bed, file = paste0(outDir, "/intersects/", tmp_lead_snp, 
                                         ".allChr.", annot_name, ".intersect.sorted.filtered.bed"), 
                  quote = F, row.names  = F, col.names = F, sep = "\t")
      
    }
  #}
}

get_median_and_distribute = function(outDir) {
  # get the median evol measure from each set.
  # plot the distribution, run a test between CSA-region evol. score and
  # the rest. see if it's significantly high/low.
  
  csa_regions_summary = data.frame(lead_snp = character(),
                                   z_score = numeric(),
                                   pval = numeric(),
                                   mean = numeric(),
                                   median = numeric(),
                                   std = numeric(),
                                   lead_snp_percentile = numeric(),
                                   test_type = character())
  lead_snp_count = 1
  for (f in dir(paste0(outDir, "/intersects/"), pattern = ".intersect.sorted.filtered.bed", full.names = T)) {
    tmp_bed = read.table(f)
    median_df = data.frame(set_n = numeric(), 
                           median_score = numeric())
    
    k = 1
    for (set_n in levels(as.factor(tmp_bed$V5))) {
      median_df[k,1] = as.numeric(set_n)
      median_df[k,2] = median(tmp_bed[tmp_bed$V5==set_n,]$V6)
      k = k + 1
    }
    
    median_df = na.omit(median_df)
    hist(median_df$median_score)
    
    z_score = (median_df[median_df$set_n=="1",]$median_score - mean(median_df$median_score)) / sd(median_df$median_score)
    
    control_vars = median_df[-1,]
    percentile = ecdf(control_vars$median_score)
    lead_snp_percentile = percentile(median_df$median_score[1])
    
    pvalue = nrow(control_vars[control_vars$median_score >= median_df$median_score[1],]) / nrow(control_vars)
    
    if (pvalue > 0.5) {
      pvalue = nrow(control_vars[control_vars$median_score <= median_df$median_score[1],]) / nrow(control_vars)
      pvalue = 2 * pvalue
      test_type = "two-tailed"
    }
    else {
      test_type = "greater-than"
    }

    csa_regions_summary[lead_snp_count,1] = strsplit(strsplit(f, split = "/")[[1]][16], split = "\\.")[[1]][1]
    csa_regions_summary[lead_snp_count,2] = z_score
    csa_regions_summary[lead_snp_count,3] = pvalue
    csa_regions_summary[lead_snp_count,4] = mean(median_df$median_score)
    csa_regions_summary[lead_snp_count,5] = median(median_df$median_score)
    csa_regions_summary[lead_snp_count,6] = sd(median_df$median_score)
    csa_regions_summary[lead_snp_count,7] = lead_snp_percentile
    csa_regions_summary[lead_snp_count,8] = test_type

    lead_snp_count = lead_snp_count + 1
  }
  
  write.table(csa_regions_summary, paste0(outDir, "/selection_analysis_summary.txt"),
              quote = F, row.names = F, col.names = F, sep = "\t")
  
}

#--------------------------------------------
# MAIN

###
# Read in clumped SNPs from all summary stats
# and merge them.
###

#df2bed(CSAregions, outDir)

#--------------------------------------------

sessionInfo()